# 第一周总结

## 单变量线性回归：

$$
y = \theta_0 + \theta_1x
$$



## 代价函数：

$$
J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x^i-y^i)^2
$$



## 参数优化：

寻找$\theta_0$和$\theta_1$使得$J(\theta_0, \theta_1)$最小。



## 思路：

随机选取$\theta_0$和$\theta_1$，默认均选择0，反复更新$\theta_0$和$\theta_1$，使得$J(\theta_0, \theta_1)$最小。



## 更新公式：

$$
\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)
$$

其中$\alpha$是学习速率，即梯度下降的步伐大小。后面是$J(\theta_0, \theta_1)$的导数。



## 梯度下降的线性回归算法

**当j=0时，$\frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)$**

$\theta_0$导数部分推导过程：利用导数性质$y=x^n$的导数为$y'=nx^{n-1}$可得以下推导过程
$$
\frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1)=\frac{\partial}{\partial\theta_0}(\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2)\\
=\frac{\partial}{\partial\theta_0}(\frac{1}{2m}\sum_{i=1}^{m}(\theta_0 + \theta_1x^i-y^i)^2)\\
=\frac{1}{2m}\times2\times\sum_{i=1}^m(\theta_0+\theta_1x^i-y^i)\\
=\frac{1}{2m}\times2\times\sum_{i=1}^m(h_\theta(x^i)-y^i)\\
=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i)-y^i)
$$



**当j=1时，$\frac{\partial}{\partial\theta_1}J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)\times x^i$**

$\theta_1$导数部分推导过程：
$$
\frac{\partial}{\partial\theta_1}J(\theta_0, \theta_1)=\frac{\partial}{\partial\theta_1}(\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2)\\
=\frac{\partial}{\partial\theta_1}(\frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x^i-y^i)^2)\\
=\frac{1}{2m}\sum_{i=1}^{m}(x^i\times(\theta_0+\theta_1x^i-y^i)+x^i\times(\theta_0+\theta_1x^i-y^i))\\
=\frac{1}{2m}\sum_{i=1}^{m}(2\times x^i \times(\theta_0+\theta_1x^i-y^i))\\
=\frac{1}{m}\times x^i\times\sum_{i=1}^{m}(\theta_0+\theta_1x^i-y^i)\\
=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)\times x^i
$$


