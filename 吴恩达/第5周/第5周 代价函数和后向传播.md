# 第5周 代价函数和后向传播

## 一、代价函数

首先定义几个变量：

+ L：神经网络的层数（包含了输入和输出层）

+ $s_l$：第$l$层的激活节点数，不包含偏差节点

+ K：输出的分类数

+ $h_\Theta(x)_k$：假设函数在第k个输出节点的结果

+ $\theta^k_{ij}$：第$k$层的第$j$个输入到第$k+1$层的第$i$个激活节点

  ***k***：代表第***k***到***k+1***层的映射。

  ***i***：代表第***k+1***层的第***i***个激活节点。（不包含偏差节点）

  ***j***：代表第***k***层的第***j***个输入。（包含偏差节点）
  
  $\Theta^{层数}_{目标节点，输入节点}$

之前逻辑回归的代价函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta^2_j
$$
而神经网络的代价函数是逻辑回归函数的泛化，如下，其中$h_\Theta(x)_i$表示第i个输出：
$$
J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right]\\ + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2
$$
这里添加一些嵌套求和，以说明我们的多个输出节点。在公式的第一部分中，在方括号之前，有一个附加的嵌套求和，该求和遍历输出节点的数量。

在正则化部分中，即方括号之后，我们必须考虑多个$\theta$矩阵。当前$\theta$矩阵中的列数等于当前层（包含偏差单元）中的节点数。当前$\theta$矩阵中的行数等于下一层的节点数（不包含偏差单元）。与之前的逻辑回归一样，我们对每个项目都求平方。

注意：

+ 嵌套求和只是为了将输出层中的每个单元的逻辑回归成本相加。

+ 三重嵌套求和只是为了将整个神经网络中所有单独$\theta$的平方相加。并不是真正的三次和相加。

  

## 二、反向传播算法

反向传播是最小化神经网络代价函数的一个算法，正如我们在逻辑回归和线性回归代价函数中所做的那样，我们的目的是最小化$J(\Theta)$的值。也就是说，我们想用最优的$\Theta$参数集来最小化代价函数$J$。于是，问题就转换为如何求解$J(\Theta)$的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$。

给定训练集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}),\cdots,(x^{(m)},y^{(m)})}$，设对每一个$(l,i,j)$，有$\Delta^{(l)}_{i,j}=0$，接下来根据训练集进行计算：

1. 设$a^{(1)}:=x^{(t)}$，$t=1\cdots m$

2. 前向传播计算$a^{(l)}$的值，$l=2,3,\cdots,L$。L为神经网络的层数（含输入层和输出层）。$a^{(l)}$为第l层的激活节点（经过sigmoid函数计算）。对于四层的神经网络，有：
   $$
   \begin{align*}
   a^{(1)}&=x\\
   z^{(2)}&=\Theta^{(1)}a^{(1)}\\
   a^{(2)}&=g(z^{(2)}+a^{(2)}_0)\\
   z^{(3)}&=\Theta^{(2)}a^{(2)}\\
   a^{(3)}&=g(z^{(3)}+a^{(3)}_0)\\
   z^{(4)}&=\Theta^{(3)}a^{(3)}\\
   \end{align*}
   $$

3. 计算输出层的误差：$\delta^{(L)}=a^{(L)}-y^{(t)}$。

4. 计算第2到L-1层的误差：$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}\cdot a^{(l)}\cdot(1-a^{(l)})$

5. 更新$\Delta^{(l)}_{i,j}$，$\Delta^{(l)}_{i,j}:=\Delta^{(l)}_{i,j}+a_j^{(l)}\delta_i^{(l+1)}$。

最终$J(\Theta)$的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$可写为：
$$
\begin{align}
\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)=D^{(l)}_{i,j}&=\frac{1}{m}(\Delta^{(l)}_{i,j}+\lambda\Theta^{(l)}_{i,j}),if\space j\neq0\\
\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)=D^{(l)}_{i,j}&=\frac{1}{m}\Delta^{(l)}_{i,j},if\space j=0
\end{align}
$$


第4步的$\delta^{(l)}$是第$l$层的误差，即目标值对该层的激活节点值求偏导数，推导过程如下：

首先需定义一个总误差的概念：$E_{total}=\frac{1}{2}\sum_{i=1}^m(h(x_i)-y_i)^2$

第L层（输出层）的误差：$\delta^{(L)}=a^{(L)}-y^{(t)}$。

第L-1层的某一个激活节点的误差为：
$$
\begin{align*}
\delta^{(L-1)}&=\frac{\partial E}{\partial a^{(L-1)}}\\
&=\frac{\partial E}{\partial a^{(L)}}\times\frac{\partial a^{(L)}}{\partial z^{(L)}}\times\frac{\partial z^{(L)}}{\partial a^{(L-1)}}\\
&=(\frac{1}{2}(a^{(L)}-y^{t})^2)'\times(\frac{1}{1+e^{-z^{(L)}}})'\times((\Theta^{(L-1)})^Ta^{(L-1)})'\\
&=(a^{(L)}-y^{(t)})\times(a^{(L)})(1-a^{(L)})\times(\Theta^{(L-1)})^T\\
&=\delta^{(L)}\times(a^{(L)})(1-a^{(L)})\times(\Theta^{(L-1)})^T

\end{align*}
$$
