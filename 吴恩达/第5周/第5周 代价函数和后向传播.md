# 第5周 代价函数和后向传播

## 一、代价函数

首先定义几个变量：

+ L：神经网络的层数（包含了输入和输出层）

+ $s_l$：第$l$层的激活节点数，不包含偏差节点

+ K：输出的分类数

+ $h_\Theta(x)_k$：假设函数在第k个输出节点的结果

+ $\theta^k_{ij}$：第$k$层的第$j$个输入到第$k+1$层的第$i$个激活节点

  ***k***：代表第***k***到***k+1***层的映射。

  ***i***：代表第***k+1***层的第***i***个激活节点。（不包含偏差节点）

  ***j***：代表第***k***层的第***j***个输入。（包含偏差节点）
  
  $\Theta^{层数}_{目标节点，输入节点}$

之前逻辑回归的代价函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta^2_j
$$
而神经网络的代价函数是逻辑回归函数的泛化，如下，其中$h_\Theta(x)_i$表示第i个输出：
$$
J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right]\\ + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2
$$
这里添加一些嵌套求和，以说明我们的多个输出节点。在公式的第一部分中，在方括号之前，有一个附加的嵌套求和，该求和遍历输出节点的数量。

在正则化部分中，即方括号之后，我们必须考虑多个$\theta$矩阵。当前$\theta$矩阵中的列数等于当前层（包含偏差单元）中的节点数。当前$\theta$矩阵中的行数等于下一层的节点数（不包含偏差单元）。与之前的逻辑回归一样，我们对每个项目都求平方。

注意：

+ 嵌套求和只是为了将输出层中的每个单元的逻辑回归成本相加。

+ 三重嵌套求和只是为了将整个神经网络中所有单独$\theta$的平方相加。并不是真正的三次和相加。

  

## 二、反向传播算法

反向传播是最小化神经网络代价函数的一个算法，正如我们在逻辑回归和线性回归代价函数中所做的那样，我们的目的是最小化$J(\Theta)$的值。也就是说，我们想用最优的参数集来最小化代价函数$J$。于是，问题就转换为如何求解$J(\Theta)$的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$。

计算过程：

给定训练集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}),\cdots,(x^{(m)},y^{(m)})}$，设对每一个$(l,i,j)$

