# 第3周 分类、拟合、正规化

## 一、分类

线性回归不适用于分类问题。

分类问题是把要预测的值归类到几个离散值。在本周，我们只讨论二元分类问题，即预测值只有0和1两个结果分类。

## 二、假设公式

逻辑函数、S型函数
$$
h_\theta(x)=g(\theta^Tx)
$$

$$
z=\theta^Tx
$$

$$
g(z)=\frac{1}{1+e^{-z}}
$$

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

该函数的图像如下：

![img](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/1WFqZHntEead-BJkoDOYOw_2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png?expiry=1597708800000&hmac=mK6jKwrI-Vy1hAiDuUJytsTyNRiLvXOGMbXF7m6MnJo)

重要分界点：
$$
z=0,h_\theta(x)=0.5\\
z=5,h_\theta(x)=0.9933\\
z=7,h_\theta(x)=0.999\\
z=10,h_\theta(x)=0.9999\\
z=21,h_\theta(x)=0.999999998
$$


该函数将任意实数映射到(0,1)这个区间，从而告诉我们相应分类的概率情况。
$$
h_\theta(x) = P(y=1|x;\theta)=1-P(y=0|x;\theta)
$$

$$
P(y=0|x;\theta)+P(y=1|x;\theta)=1
$$

## 三、决策边界

因为假设公式里面，当$z\geq0$，则$g(z)\geq0.5$。因此，为了把上述假设公式的结果分类到0和1，我们可以进行下面的转换：
$$
h_\theta(x)\geq0.5\rightarrow y=1\\
h_\theta(x)\lt0.5\rightarrow y=0
$$
推导过程：
$$
z=0,e^{-0}=1\Rightarrow g(z)=\frac{1}{1+1}=0.5\\
z\rightarrow\infty, e^{-\infty}\rightarrow0\Rightarrow g(z)=\frac{1}{1+0}=1\\
z\rightarrow-\infty, e^{\infty}\rightarrow\infty\Rightarrow g(z)=\frac{1}{1+\infty}=0
$$
因为$z=\theta^Tx$，所以当$\theta^Tx\geq0$时，$1\geq h_\theta(x)= g(\theta^Tx)\geq0.5$。

因此我们可以得到：
$$
\theta^Tx\geq0\Rightarrow y=1\\
\theta^Tx\lt0\Rightarrow y=0
$$


决策边界就是根据我们的假设公式得到的区分$y=0$和$y=1$的直线。

举个例子：

当$\theta=\left[\begin{matrix}5 \\-1\\0\end{matrix}\right]$时，且$5+（-1）x_1+0x_2\geq0$，$y=1$，此时$5-x_1\geq0, x_1\leq5$。

而$x=5$这条直线就是决策边界，位于其左侧的取值都会使$y=1$，位于其右侧的取值都会使$y=0$。

最后补充：$\theta^Tx$的函数不需要是一个线性函数，可以是任意形状的函数。



## 四、代价函数

分类问题的逻辑函数不能使用线性回归的代价函数，因为逻辑函数将导致输出的图形呈现波浪形状，会有多个局部最优解。也就是说，它不是一个凸函数。

取而代之的是，逻辑函数的代价函数如下所示：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}, y^{(i)}))\\

Cost(h_\theta(x^{(i)}, y^{(i)}))=-log(h_\theta(x)) \space\space\space\space\space\space\space\space\space如果y=1\\
Cost(h_\theta(x^{(i)}, y^{(i)}))=-log(1-h_\theta(x))\space\space如果y=0
$$
当$y=1$时，$J(\theta)$的图像是从无穷大到0逐渐递减到0的。当预测值是0，而实际值是1时，$Cost(h_\theta(x), y)\rightarrow\infty$。

当$y=0$时，$J(\theta)$的图像是从0逐渐递增到无限大的。当预测值是1，而实际值是0时，$Cost(h_\theta(x), y)\rightarrow\infty$。

当预测值刚好等于实际值时，即$h_\theta(x)=y$时，$Cost(h_\theta(x), y)=0$。

## 五、简化的代价函数和梯度下降

上节的两个代价函数可以合并成一个式子：
$$
Cost(h_\theta(x), y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$

因此我们可以写出完整的代价函数表达式为：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))
$$

### 5.1 梯度下降计算参数

重复计算各个参数直到收敛：
$$
\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
$$
通过计算导数可以得到：
$$
\theta_j:=\theta_j-\frac{\alpha}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$

导数计算过程：

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))\\
$$
要对$J(\theta)$求导，先把常数项$-\frac{1}{m}\sum_{i=1}^m$取出来，对剩余部分求导即可。

根据对数复合求导公式：$log(x)'=\frac{1}{x}x'$，可得剩余部分的求导等于：
$$
y^{(i)}\frac{1}{h_\theta(x^{(i)})}h_\theta(x^{(i)})'+(1-y)\frac{1}{1-h_\theta(x^{(i)})}(1-h_\theta(x^{(i)}))'\\
$$
先求解$h_\theta(x^{(i)})'$的值，首先根据$(\frac{u}{v})'=\frac{u'v-uv'}{v^2}$，可得到：
$$
h_\theta(x^{(i)})'=(\frac{1}{1+e^{-\theta^Tx^{(i)}}})'\\
=\frac{1'\times(1+e^{-\theta^Tx^{(i)}})-1\times(1+e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}\\
=-\frac{(1+e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}
$$
根据$(u\pm v)'=u'\pm v'$，可得到：
$$
-\frac{(1+e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}=-\frac{(e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}\\
$$
根据$(e^{f(x)})'=f'(x)e^{f(x)}$，可得到：
$$
-\frac{(e^{-\theta^Tx^{(i)})'}}{(1+e^{-\theta^Tx^{(i)}})^2}=-\frac{(-\theta^Tx^{(i)})'\times e^{-\theta^Tx}}{(1+e^{-\theta^Tx^{(i)}})^2}\\
=\frac{e^{-\theta^Tx^{(i)}}}{(1+e^{-\theta^Tx^{(i)}})^2}\times(\theta^Tx^{(i)})'\\
=\frac{(1+e^{-\theta^Tx^{(i)}})-1}{(1+e^{-\theta^Tx^{(i)}})^2}\times(\theta^Tx^{(i)})'\\
=\frac{1}{1+e^{-\theta^Tx^{(i)}}}\times\frac{(1+e^{-\theta^Tx^{(i)}})-1}{1+e^{-\theta^Tx^{(i)}}}\times(\theta^Tx^{(i)})'\\
=\frac{1}{1+e^{-\theta^Tx^{(i)}}}\times(1-\frac{1}{1+e^{-\theta^Tx^{(i)}}})\times x^{(i)}\\
=h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}
$$

因此可得到：
$$
h_\theta(x^{(i)})'=h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}
$$
接着求解$(1-h_\theta(x^{(i)}))'$的值：
$$
(1-h_\theta(x^{(i)}))'=(1-\frac{1}{1+e^{-\theta^Tx^{(i)}}})'\\
=(\frac{e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}})'\\
=\frac{(e^{-\theta^Tx^{(i)}})'\times(1+e^{-\theta^Tx^{(i)}})-e^{-\theta^Tx^{(i)}}\times(1+e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}\\
=\frac{(-\theta^Tx^{(i)})'\times e^{-\theta^Tx^{(i)}}\times(1+e^{-\theta^Tx^{(i)}})-e^{-\theta^Tx^{(i)}}\times(e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}\\
=\frac{(-\theta^Tx^{(i)})'\times e^{-\theta^Tx^{(i)}}+(-\theta^Tx^{(i)})'\times e^{-\theta^Tx^{(i)}}\times e^{-\theta^Tx^{(i)}}-e^{-\theta^Tx^{(i)}}\times(-\theta^Tx^{i})'\times e^{-\theta^Tx^{(i)}}}{(1+e^{-\theta^Tx^{(i)}})^2}\\
=-\frac{e^{-\theta^Tx^{(i)}}+e^{-\theta^Tx^{(i)}}\times e^{-\theta^Tx^{(i)}}-e^{-\theta^Tx^{(i)}}\times e^{-\theta^Tx^{(i)}}}{(1+e^{-\theta^Tx^{(i)}})^2}\times(\theta^Tx^{(i)})'\\
=-\frac{e^{-\theta^Tx^{(i)}}}{(1+e^{-\theta^Tx^{(i)}})^2}\times(\theta^Tx^{{i}})'\\
=-\frac{1}{1+e^{-\theta^Tx^{(i)}}}\times\frac{e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}\times(\theta^Tx^{i})'\\
=-h_\theta(x^{{i}})\times(\frac{1+e^{-\theta^Tx^{(i)}}-1}{1+e^{-\theta^Tx^{(i)}}})\times(\theta^Tx^{(i)})'\\
=-h_\theta(x^{(i)})\times(\frac{1+e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}-\frac{1}{1+e^{-\theta^Tx^{(i)}}})\times(\theta^Tx^{(i)})'\\
=-h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}
$$
把公式20、21代入公式16可得：
$$
y^{(i)}\frac{1}{h_\theta(x^{(i)})}h_\theta(x^{(i)})'+(1-y)\frac{1}{1-h_\theta(x^{(i)})}(1-h_\theta(x^{(i)}))'\\
=y^{(i)}\frac{1}{h_\theta(x^{(i)})}h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)}+(1-y)\frac{1}{1-h_\theta(x^{(i)})}(-h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))x^{(i)})\\
=y^{(i)}(1-h_\theta(x^{(i)}))x^{(i)}-(1-y)h_\theta(x^{(i)})x^{(i)}\\
=(y^{(i)}-y^{(i)}h_\theta(x^{(i)})-h_\theta(x^{(i)})+y^{(i)}h_\theta(x^{(i)}))x^{(i)}\\
=(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}
$$
把之前取出的常数项代入，可得：
$$
J'(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}\\
=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}
$$
参数梯度下降公式即为：
$$
\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
=\theta_j-\frac{\alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$

## 六、高级优化方法

共轭梯度法（Conjugate gradient）、“BFGS”、“L-BFGS”是更复杂，但更快速的求解$\theta$值的方法，可以用来替代梯度下降法。不建议直接实现这些算法，而是要利用已有的库或包来调用。

这几种算法的优点在于① 不需要手工选择$\alpha$值，② 通常比梯度下降算法要快。缺点就是太复杂了。

## 七、多分类问题

我们可以构建n个逻辑回归方程来解决n分类问题。我们每次选择一个分类，然后将其他所有分类视为一个总体的其他分类，这样就可以用二元逻辑回归方程来表达。反复这样做，对每个情况应用二元逻辑回归，然后使用返回最高值的假设作为我们的预测。即对于一个输入值，分别求出它在这n个方程中的值，取最高的那个值作为其对应的分类。

## 八、拟合问题

+ 欠拟合问题：假设函数过于简单或使用的特征太少。
+ 过拟合问题：对已知数据过分拟合，但对新数据无法拟合，它通常是由复杂的函数引起的，该函数会创建许多与数据无关的不必要的曲线和角度

解决过拟合问题：

1. 减少特征数：筛选必要的特征，使用模型选择算法。
2. 正规化：保留所有特征，但减小参数$\theta$的大小。当参数都具有意义时，正规化效果很好。

## 九、正规化的代价函数

如果我们的假设函数过拟合，我们可以添加其代价来减少函数中某些项的权重。

假设我们的预测函数形如：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，如果我们要使得其更像二次方程，那就必须消除$\theta_3x^3+\theta_4x^4$在式子中的影响。除了直接删掉这两项，我们还可以对代价函数进行如下的修改：
$$
min_\theta\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+1000\times\theta_3^2+1000\times\theta_4^2
$$
$min_\theta\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+1000\times\theta_3^2+1000\times\theta_4^2$

我们在代价函数的末端添加两个项目，此时为了最小化整个式子，则必须将$\theta_3$和$\theta_4$减小到接近0，这才能使$1000\theta_3^2$和$1000\theta_4^2$接近0。这样预测函数的图像才能像二次方程的图像。

根据上面的原理，我们可以写出所有参数都正规化的代价函数：
$$
min_\theta\frac{1}{2m}(\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2)
$$
参数含义：

+ m：样本数
+ i： 第i个特征
+ j：第j个参数
+ n：特征数
+ $\lambda$：膨胀系数

通过将上述代价函数与额外的总和结合使用，我们可以平滑假设函数的输出，以减少过拟合。 

如果选择的$\lambda$太大，则可能会使功能过于平滑，从而导致欠拟合。

## 十、正规化线性回归

### 10.1 梯度下降求解

将原有的梯度下降函数分解为$\theta_0$和$\theta_j$。重复如下计算以至收敛：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha((\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})+\frac{\lambda}{m}\theta_j)\\
=(1-\alpha\frac{\lambda}{m})\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
$\frac{\lambda}{m}\theta_j$该项是用来正规化的。$1-\alpha\frac{\lambda}{m}$始终小于1，因此每次迭代$\theta_j$都是递减的。

### 10.2 正规方程

为了添加正规化，我们必须在原有方程的内部添加一个新的项目：
$$
\theta = (X^TX+\lambda\times L)^{-1}X^Ty
$$
其中，$L=\left[\begin{matrix}0&0&0&0&0\\0 & 1&0&0&0\\0 &0 & 1&0&0\\0 &0 &0 &\ddots&0\\0&0&0&0&1\end{matrix}\right]$，L是一个左上角为0，对角线为1，其余位置均为0的矩阵，它是一个$(n+1)\times(n+1)$的矩阵，其中n为特征数。

当$m<n$时，$X^TX$是不可逆的，但添加了$\lambda\times L$后，其结果是可逆的。

## 十一、正规化逻辑回归

对逻辑回归分类算法的代价函数添加一项即可实现正规化。
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}((y^{(i)}log(h_\theta(x^{(i)})-y^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta^2_j
$$
添加的这一项$\frac{\lambda}{2m}\sum_{j=1}^n\theta^2_j$，可以实现排除参数系数的偏差项。

对上述代价函数求导可得：
$$
\frac{\partial}{\partial\theta}J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}((y^{(i)}log(h_\theta(x^{(i)})-y^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))+\frac{\lambda}{m}\sum_{j=1}^{n}\theta_j
$$


因为$\theta$参数向量索引从$0$到$n$，而这一项是从1开始的，因为在计算的时候，我们要重复更新2个方程：
$$
repeat\begin{cases}
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x_0^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha(\frac{1}{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j)\\
\end{cases}
$$


