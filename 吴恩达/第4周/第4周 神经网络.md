# 第4周 神经网络

## 一、模型表示

用神经网络来表示假设函数。在一个非常简单的层面上，一个计算节点就是一个神经元，它将输入(树突)作为电输入(称为“尖峰”)，然后输送到输出(轴突)。

在我们的模型中，树突就像输入特征$x_1\cdots x_n$，输出是假设函数的结果。

在这个模型中，我们的$x_0$输入节点有时被称为“偏置节点”。它总是等于1。

在神经网络中，我们使用与分类模型中的逻辑回归一样的函数，我们有时称其为s型(logistic)激活函数。$\frac{1}{1+e^{-\theta^Tx}}$。在这个神经网络中，我们的$\theta$参数有时称为权重。

神经网络一个表现形式如下：
$$
[x_0x_1x_2]\rightarrow[\cdots]\rightarrow h_\theta(x)
$$
第一层称为输入层，输入到中间的隐藏层，再输出到输出层。

输入层和输出层之间可以有多个中间层（隐藏层）。

在下面这个例子中，我们把中间层（隐藏层）的节点记为$a^2_0\cdots a_n^2$，并称其为激活节点。

$a_i^{(j)}$：表示第***j***层的第***i***个激活节点。

$\theta^{(j)}$：表示从第***j***层映射到第***j+1***层的函数权重的矩阵。

如果我们有一个隐藏层，那么神经网络的表现形式如下：
$$
[x_0x_1x_2x_3]\rightarrow[a_1^{(2)}a_2^{(2)}a_3^{(2)}]\rightarrow h_\theta(x)
$$
上面的式子表示隐藏层有3个激活节点。每个激活节点的值如下：
$$
a^{(2)}_1=g(\theta^{(1)}_{10}x_0+\theta^{(1)}_{11}x_1+\theta^{(1)}_{12}x_2+\theta^{(1)}_{13}x_3)\\
a^{(2)}_2=g(\theta^{(1)}_{20}x_0+\theta^{(1)}_{21}x_1+\theta^{(1)}_{22}x_2+\theta^{(1)}_{23}x_3)\\
a^{(2)}_3=g(\theta^{(1)}_{30}x_0+\theta^{(1)}_{31}x_1+\theta^{(1)}_{32}x_2+\theta^{(1)}_{33}x_3)\\
h_\theta(x)=a_1^{(3)}=g(\theta^{(2)}_{10}a^{(2)}_0+\theta^{(2)}_{11}a^{(2)}_1+\theta^{(2)}_{12}a^{(2)}_2+\theta^{(2)}_{13}a^{(2)}_3)
$$
**$\theta^k_{ij}$说明：**

***k***：代表第***k***到***k+1***层的映射。

***i***：代表第***k+1***层的第***i***个激活节点。

***j***：代表第***k***层的第***j***个输入。

这就是说，我们通过使用由$3\times4$个参数组成的矩阵来计算我们的激活节点。我们将每行参数应用于输入，以获得一个激活节点的值。

我们将权重矩阵的每行参数应用于输入来计算一个激活节点的值。我们的假设函数的输出，是把各个激活节点的值乘以相应的权重矩阵之后的和应用到逻辑回归的函数上。

每一层都有属于该层的权重矩阵$\theta^{(j)}$。

这些权重矩阵的维度由以下几个因素决定：

如果神经网络的第$j$层有$S_j$个节点，第$j+1$层有$S_{j+1}$个节点，那么$\theta^{(j)}$的维度是$S_{j+1}\times(S_j+1)$，其中$+1$是偏置节点$x_0$和其权重$\theta^{(j)}_0$。换句话说，输出节点不包含偏置节点，而输入将包含偏置节点。

接下来我们对公式3进行向量化实现，我们定义一个新变量$z_k^{(j)}$，让它来表示g函数里面的值，即：
$$
a_1^{(2)}=g(z^{(1)}_1)\\
a_2^{(2)}=g(z^{(1)}_2)\\
a_3^{(2)}=g(z^{(1)}_3)
$$
也就是说，对于第***j=2***层的节点***k***，变量***z***表示为：
$$
z_k^{(2)}=\theta_{k,0}^{(1)}x_0+\theta_{k,1}^{(1)}x_1+...+\theta_{k,n}^{(1)}x_n
$$
$x$和$z^j$的向量表示如下：
$$
x=\left[\begin{matrix}x^0\\x^1\\\vdots\\x^n\end{matrix}\right]\\
z^j=\left[\begin{matrix}z_1^{(j)}\\z_2^{(j)}\\\vdots\\z_n^{(j)}\end{matrix}\right]
$$
设$x=a^{(1)}$，我们可以把公式（5）重写为：
$$
z^{(j)}=\theta^{(j-1)}a^{(j-1)}
$$
我们将矩阵$\theta^{(j-1)}$乘以向量$a^{(j-1)}$（其维度为：$s_j\times(n+1)$）。这样我们可以得到一个高度为$s_j$的$z^{j}$向量。因此，激活节点可以表示为：$a^{(j)}=g(z^{j})=\theta^{(j-1)}a^{(j-1)}$。

接着，我们可以添加偏执节点（等于1）到第***j***层，然后计算第***j+1***层的节点，以此类推。

最后的的矩阵$\theta^{(j)}$只有一行，乘以只有一列的$a^{(j)}$，得到一个唯一的实数结果。
$$
h_\theta(x)=a^{(j+1)}=g(z^{(j+1)})
$$
注意，在最后一步，在第j层和第j+1层之间，我们做的事情和逻辑回归中做的完全一样。在神经网络中加入所有这些中间层可以让我们更优雅地产生有趣而复杂的非线性假设。

## 二、实例和经验

### 2.1 "逻辑与"（AND）操作

真值表

| $x_1$ | $x_2$ | result |
| :---: | :---: | :----: |
|   0   |   0   |   0    |
|   0   |   1   |   0    |
|   1   |   0   |   0    |
|   1   |   1   |   1    |

神经网络一个简单的例子是实现“逻辑与”操作。即仅当$x_1$和$x_2$均为1时，才输出1。此时我们的函数表示如下：
$$
\left[\begin{matrix}x_0\\x_1\\x_2\end{matrix}\right]\rightarrow[g(z^{(2)})]\rightarrow h_\Theta(x)
$$
其中$x_0=1$。

接下来看权重矩阵$\Theta^{(1)}=[-30, 20, 20]$，于是我们的函数变为：
$$
h_\Theta(x)=g(-30+20x_1+20x_2)
$$
由于$x_1,x_2$的取值范围是$[0, 1]$。于是函数的结果有：
$$
x_1=0,x_2=0\rightarrow h_\Theta(x)=g(-30)=\frac{1}{1+e^{30}}\approx0\\
x_1=0,x_2=1\rightarrow h_\Theta(x)=g(-10)=\frac{1}{1+e^{10}}\approx0\\
x_1=1,x_2=0\rightarrow h_\Theta(x)=g(-10)=\frac{1}{1+e^{10}}\approx0\\
x_1=1,x_2=1\rightarrow h_\Theta(x)=g(10)=\frac{1}{1+e^{-10}}\approx1
$$


### 2.2 “逻辑或”（OR）操作

真值表

| $x_1$ | $x_2$ | result |
| :---: | :---: | :----: |
|   0   |   0   |   0    |
|   0   |   1   |   1    |
|   1   |   0   |   1    |
|   1   |   1   |   1    |

取权重矩阵$\Theta^{(1)}=[-10, 20, 20]$，于是我们的函数变为：
$$
h_\Theta(x)=g(-10+20x_1+20x_2)
$$
由于$x_1,x_2$的取值范围是$[0, 1]$。于是函数的结果有：
$$
x_1=0,x_2=0\rightarrow h_\Theta(x)=g(-10)=\frac{1}{1+e^{30}}\approx0\\
x_1=0,x_2=1\rightarrow h_\Theta(x)=g(10)=\frac{1}{1+e^{-10}}\approx1\\
x_1=1,x_2=0\rightarrow h_\Theta(x)=g(10)=\frac{1}{1+e^{-10}}\approx1\\
x_1=1,x_2=1\rightarrow h_\Theta(x)=g(30)=\frac{1}{1+e^{-30}}\approx1
$$

### 2.3 "逻辑或非"（NOR）操作

真值表

| $x_1$ | $x_2$ | result |
| :---: | :---: | :----: |
|   0   |   0   |   1    |
|   1   |   0   |   0    |
|   0   |   1   |   0    |
|   1   |   1   |   0    |

取权重矩阵$\Theta^{(1)}=[10, -20, -20]$，于是我们的函数变为：
$$
h_\Theta(x)=g(10-20x_1-20x_2)
$$
由于$x_1,x_2$的取值范围是$[0, 1]$。于是函数的结果有：
$$
x_1=0,x_2=0\rightarrow h_\Theta(x)=g(10)=\frac{1}{1+e^{-10}}\approx1\\
x_1=0,x_2=1\rightarrow h_\Theta(x)=g(-10)=\frac{1}{1+e^{10}}\approx0\\
x_1=1,x_2=0\rightarrow h_\Theta(x)=g(-10)=\frac{1}{1+e^{10}}\approx0\\
x_1=1,x_2=1\rightarrow h_\Theta(x)=g(-30)=\frac{1}{1+e^{30}}\approx0
$$

### 2.4 “逻辑同或”（XNOR）操作

真值表

| $x_1$ | $x_2$ | result |
| :---: | :---: | :----: |
|   0   |   0   |   1    |
|   0   |   1   |   0    |
|   1   |   0   |   0    |
|   1   |   1   |   1    |

实现“逻辑同或”需要利用到“逻辑与”和“逻辑或非”，再对这两个结果进行“逻辑或”，因此这里需要两层神经网络，第一层有两个激活节点，其权重矩阵为：
$$
\Theta^{(1)}=\left[\begin{matrix}-30,&20,&20\\10,&-20,&-20\end{matrix}\right]
$$
第二层需要一个激活节点，其权重矩阵为：
$$
\Theta^{(2)}=\left[\begin{matrix}-10,&20,&20\end{matrix}\right]
$$
于是我们的函数的计算过程为：
$$
a^{(2)}=g(\Theta^{(1)} \times x)\\
a^{(3)}=g(\Theta^{(2)} \times a^{(2)})\\
h_\Theta(x)=a^{(3)}
$$
计算过程：

| $x_1$ | $x_2$ | 逻辑与 | 逻辑或非 | 结果（逻辑或） |
| :---: | :---: | :----: | :------: | :------------: |
|   0   |   0   |   0    |    1     |       1        |
|   0   |   1   |   0    |    0     |       0        |
|   1   |   0   |   0    |    0     |       0        |
|   1   |   1   |   1    |    0     |       1        |

