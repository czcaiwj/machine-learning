# 第三周笔记

## 分类

线性回归不适用于分类问题。

分类问题是把要预测的值归类到几个离散值。在本周，我们只讨论二元分类问题，即预测值只有0和1两个结果分类。

## 假设公式

逻辑函数、S型函数
$$
h_\theta(x)=g(\theta^Tx)
$$

$$
z=\theta^Tx
$$

$$
g(z)=\frac{1}{1+e^{-z}}
$$

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

该函数的图像如下：

![img](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/1WFqZHntEead-BJkoDOYOw_2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png?expiry=1597708800000&hmac=mK6jKwrI-Vy1hAiDuUJytsTyNRiLvXOGMbXF7m6MnJo)

该函数将任意实数映射到(0,1)这个区间，从而告诉我们相应分类的概率情况。
$$
h_\theta(x) = P(y=1|x;\theta)=1-P(y=0|x;\theta)
$$

$$
P(y=0|x;\theta)+P(y=1|x;\theta)=1
$$

## 决策边界

因为假设公式里面，当$z\geq0$，则$g(z)\geq0.5$。因此，为了把上述假设公式的结果分类到0和1，我们可以进行下面的转换：
$$
h_\theta(x)\geq0.5\rightarrow y=1\\
h_\theta(x)\lt0.5\rightarrow y=0
$$
推导过程：
$$
z=0,e^{-0}=1\Rightarrow g(z)=\frac{1}{1+1}=0.5\\
z\rightarrow\infty, e^{-\infty}\rightarrow0\Rightarrow g(z)=\frac{1}{1+0}=1\\
z\rightarrow-\infty, e^{\infty}\rightarrow\infty\Rightarrow g(z)=\frac{1}{1+\infty}=0
$$
因为$z=\theta^Tx$，所以当$\theta^Tx\geq0$时，$1\geq h_\theta(x)= g(\theta^Tx)\geq0.5$。

因此我们可以得到：
$$
\theta^Tx\geq0\Rightarrow y=1\\
\theta^Tx\lt0\Rightarrow y=0
$$


决策边界就是根据我们的假设公式得到的区分$y=0$和$y=1$的直线。

举个例子：

当$\theta=\left[\begin{matrix}5 \\-1\\0\end{matrix}\right]$时，且$5+（-1）x_1+0x_2\geq0$，$y=1$，此时$5-x_1\geq0, x_1\leq5$。

而$x=5$这条直线就是决策边界，位于其左侧的取值都会使$y=1$，位于其右侧的取值都会使$y=0$。

最后补充：$\theta^Tx$的函数不需要是一个线性函数，可以是任意形状的函数。



## 代价函数

分类问题的逻辑函数不能使用线性回归的代价函数，因为逻辑函数将导致输出的图形呈现波浪形状，会有多个局部最优解。也就是说，它不是一个凸函数。

取而代之的是，逻辑函数的代价函数如下所示：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}, y^{(i)}))\\

Cost(h_\theta(x^{(i)}, y^{(i)}))=-log(h_\theta(x)) \space\space\space\space\space\space\space\space\space如果y=1\\
Cost(h_\theta(x^{(i)}, y^{(i)}))=-log(1-h_\theta(x))\space\space如果y=0
$$
当$y=1$时，$J(\theta)$的图像是从无穷大到0逐渐递减到0的。当预测值是0，而实际值是1时，$Cost(h_\theta(x), y)\rightarrow\infty$。

当$y=0$时，$J(\theta)$的图像是从0逐渐递增到无限大的。当预测值是1，而实际值是0时，$Cost(h_\theta(x), y)\rightarrow\infty$。

当预测值刚好等于实际值时，即$h_\theta(x)=y$时，$Cost(h_\theta(x), y)=0$。

## 简化的代价函数和梯度下降

上节的两个代价函数可以合并成一个式子：
$$
Cost(h_\theta(x), y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$

因此我们可以写出完整的代价函数表达式为：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+log(1-h_\theta(x^{(i)}))-y^{(i)}log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(log(h_\theta(x^{(i)}))-log(1-h_\theta(x^{(i)})))+log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(log\frac{h_\theta(x^{(i)})}{1-h_\theta(x^{(i)})})+log(1-h_\theta(x^{(i)})))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(log\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})+log(1-\frac{1}{1+e^{-\theta^Tx}}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(log\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})+log(\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(log\frac{1}{e^{-\theta^Tx}})+log(\frac{\frac{1}{e^{\theta^Tx}}}{1+\frac{1}{e^{\theta^Tx}}}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(loge^{\theta^Tx})+log(\frac{\frac{1}{e^{\theta^Tx}}}{\frac{1+e^{\theta^Tx}}{e^{\theta^Tx}}}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}({\theta^Tx}loge)+log(\frac{1}{1+e^{\theta^Tx}}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}({\theta^Tx} \times 1)+log((1+e^{\theta^Tx})^{-1}))\\
=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}({\theta^Tx})-log(1+e^{\theta^Tx}))\\
$$

### 梯度下降计算参数

重复计算各个参数直到收敛：
$$
\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
$$
通过计算导数可以得到：
$$
\theta_j:=\theta_j-\frac{\alpha}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$

导数计算过程：

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))\\
$$
要对$J(\theta)$求导，先把常数项$-\frac{1}{m}\sum_{i=1}^m$取出来，对剩余部分求导即可。

根据对数复合求导公式：$log(x)'=\frac{1}{x}x'$，可得剩余部分的求导等于：
$$
y^{(i)}\frac{1}{h_\theta(x^{(i)})}h_\theta(x^{(i)})'+(1-y)\frac{1}{1-h_\theta(x^{(i)})}(1-h_\theta(x^{(i)}))'\\
$$
根据幂函数复合求导公式：$(y^x)'=xy^{x-1}x'$和以***e***为底指数求导公式：$y=e^x, y'=e^x$可得：
$$
h_\theta(x^{(i)})'=(\frac{1}{1+e^{-\theta^Tx}})'\\
=((1+e^{-\theta^Tx})^{-1})'\\
=\frac{1}{(1+e^{-\theta^Tx})^2}
$$


