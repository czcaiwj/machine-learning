# 第2周笔记

## 多个特征值

对于多特征值线性回归，我们可以进行以下定义：

+ $x_j^{(i)}$表示特征j在第i个训练样本的值

+ $x^{(i)}$表示第i个训练样本的特征集合

+ $m$表示训练样本个数

+ $n$表示特征数

对于多特征值的线性回归函数可以这样写：
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n
$$
我们假设$x_0=1$,那么上述式子可以写成：
$$
h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n\\
=\left[\begin{matrix}\theta_0,&\theta_1,&\theta_2,&\theta_3,\cdots,&\theta_n\end{matrix}\right]\times\left[\begin{matrix}x_0\\x_1\\x_2\\x_3\\\vdots\\x_n\end{matrix}\right]\\
=\theta^Tx
$$


## 多特征值的梯度下降

梯度下降方程本身是通用的形式，如下：
$$
重复如下运算直到收敛:\{\\
\theta_0 = \theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\times x_0^{(i)}\\
\theta_1 = \theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\times x_1^{(i)}\\
\theta_2 = \theta_2-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\times x_2^{(i)}\\
\cdots\\
\theta_n = \theta_n-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\times x_n^{(i)}\\
\}
$$

## 特征收缩、特征归一化

我们可以通过让每个输入的特征值保持在大致相同的范围内来加速梯度下降的计算过程。者是因为，在小范围内，梯度下降得很快，而当特征值变量范围太广，变得不均匀时，它会很低效率地下降到（局部）最优值。

为了防止这种情况出现，我们要做的就是修改我们的输入变量，使得其保持在一定的区间，比如：

$-1 \leq x \leq 1$或$-0.5 \leq x \leq 0.5$。

上述区间实际上并不强制一定要在1或0.5之间，这样做只是为了加速梯度下降的进程。

两个实现上述要求的技巧分别是：特征缩放和平均归一化。

特征缩放是将输入值除以输入变量的最大值），从而得到的新范围最大值为1。

平均归一化是将输入值减去平均值后再除以最大值和最小值的差。



## 确定学习速率$\alpha$

调试梯度下降：绘制一个曲线图，x轴为迭代次数，y轴为代价函数的值。如果曲线呈现上升状态，则需要减小$\alpha$的值。

自动收敛测试：定义一个阈值，如果在某次迭代之后$J(\theta)$的值与上一次的值之间的差小于这个阈值，则认为达到了收敛。这个阈值通常很小，比如$10^{-3}$，但实践中确定这个阈值还是比较困难的。

总的来说，如果$\alpha$太小，则收敛速度变小。如果$\alpha$太大，则可能曲线可能不会下降反而发散导致无法收敛。

$\alpha$的尝试范围可以从0.001、0.003、0.006、0.01、0.03、0.06、0.1、0.3、0.6、1类似这样的小步递增。



## 特征和多项式回归

我们可以有几种改进特征值和假设函数的表现形式的方式。

我们可以把多个特征值组合成一个，比如把特征值$x_1$和$x_2$进行组合得到$x_3 = x_1 \times x_2$。

如果直线无法拟合数据，那我们可以引入多项式。我们可以通过将假设函数变为二次函数、三次函数或平方根函数等其他形式来改变假设函数的行为或曲线。

一旦采用多项式来拟合，那么特征值的归一化就显得非常重要，否则经过幂运算后特征值的范围会变得非常大从而导致梯度下降算法计算效率变低。

## 正规方程

梯度下降提供了一种最小化代价函数的方式。有另外一种方式可以通过计算的方式直接得到最小化值，而不需要通过迭代算法。

在正规方程中，我们可以通过将$\theta_j$的导数置为0来最小化$j$。这允许我们不需要迭代就可以找到最优解。正规方程公式如下：
$$
\theta = (X^TX)^{-1}X^Ty
$$
**构建过程：**假设有m个样本，每个样本有n个特征值，那么对于样本$x^{(i)}$，其特征值可以表示为：$x^{(i)}=\left[\begin{matrix}x^{(i)}_0\\x^{(i)}_1\\x^{(i)}_2\\\vdots\\x^{(i)}_n\end{matrix}\right]$，其中$x^{(i)}_0=1, i=[1,m]$。那么我们构建一个矩阵X，形如：$\left[\begin{matrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\\\end{matrix}\right]$，而y则是样本结果向量，形如：$\vec{y}=\left[\begin{matrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{matrix}\right]$

在正规方程中，不需要对特征值进行缩放或归一化。

正规方程和梯度下降的优缺点对比如下表：

| 梯度下降                   | 正规方程                                          |
| -------------------------- | ------------------------------------------------- |
| 需要选择$\alpha$值         | 不需要选择$\alpha$值                              |
| 需要多次迭代               | 不需要迭代                                        |
| 性能为$\Omicron(kn^2)$     | 性能为$\Omicron(n^3)$，因为需要计算$X^TX$的逆矩阵 |
| 当特征值很多时可以正常使用 | 当特征值很多时效率很低                            |

在实践中，当特征值n的数量大于10000时，最好使用梯度下降算法。

## 如果正规方程中的$(X^TX)$不可逆

+ 使用python或其他程序包中的pinv支持对这样的结果进行伪逆求解
+ 裁剪特征，比如删除某些很相似的特征值
+ 当特征数≥样本数时，删除某些特征或者使用正则化

解决上述问题的方法包括删除与另一个线性相关的特征，或者在特征过多时删除一个或多个特征。